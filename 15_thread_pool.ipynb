{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多线程并发\n",
    "#### 同一时刻，Python 主程序只允许有一个线程执行，所以 Python 的并发，是通过多线程的切换完成的。\n",
    "- 事实上，Python 的解释器并不是线程安全的，为了解决由此带来的 race condition 等问题，Python 便引入了全局解释器锁，也就是同一时刻，只允许一个线程执行。当然，在执行 I/O 操作时，如果一个线程被 block 了，全局解释器锁便会被释放，从而让另一个线程能够继续执行。\n",
    "    - GIL，是最流行的 Python 解释器 CPython 中的一个技术术语。它的意思是全局解释器锁，本质上是类似操作系统的 Mutex。每一个 Python 线程，在 CPython 解释器中执行时，都会先锁住自己的线程，阻止别的线程执行。\n",
    "    - CPython 会做一些小把戏，轮流执行 Python 线程。这样一来，用户看到的就是“伪并行”——Python 线程在交错执行，来模拟真正并行的线程。\n",
    "    - CPython 引进 GIL 其实主要就是这么两个原因:\n",
    "        - 一是设计者为了规避类似于内存管理这样的复杂的竞争风险问题（race condition）；\n",
    "        - 二是因为 CPython 大量使用 C 语言库，但大部分 C 语言库都不是原生线程安全的（线程安全会降低性能和增加复杂度）。\n",
    "    - CPython 中还有另一个机制，叫做 check_interval，意思是 CPython 解释器会去轮询检查线程 GIL 的锁住情况。每隔一段时间，Python 解释器就会强制当前线程去释放 GIL，这样别的线程才能有执行的机会。\n",
    "- 如何绕过 GIL？Python 的 GIL，是通过 CPython 的解释器加的限制。如果你的代码并不需要 CPython 解释器来执行，就不再受 GIL 的限制。\n",
    "    - 很多高性能应用场景都已经有大量的 C 实现的 Python 库，例如 NumPy 的矩阵运算，就都是通过 C 来实现的，并不受 GIL 影响。\n",
    "    - 绕过 CPython，使用 JPython（Java 实现的 Python 解释器）等别的实现；\n",
    "\n",
    "- with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor\n",
    "- executor.map([function], [input])\n",
    "\n",
    "\n",
    "### 多进程并行\n",
    "- with concurrent.futures.ProcessPoolExecutor(max_workers=5) as executor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching https://movie.douban.com/cinema/later/beijing/: 403 Client Error: Forbidden for url: https://sec.douban.com/b?r=https%3A%2F%2Fmovie.douban.com%2Fcinema%2Flater%2Fbeijing%2F\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     65\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://movie.douban.com/cinema/later/beijing/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 66\u001b[0m     \u001b[43mcrawl_movie\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 39\u001b[0m, in \u001b[0;36mcrawl_movie\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcrawl_movie\u001b[39m(url):\n\u001b[1;32m     38\u001b[0m     init_page \u001b[38;5;241m=\u001b[39m fetch_content(url)\n\u001b[0;32m---> 39\u001b[0m     init_soup \u001b[38;5;241m=\u001b[39m \u001b[43mbs4\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlxml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     movie_names, urls_to_fetch, movie_dates, pages \u001b[38;5;241m=\u001b[39m [], [], [], []\n\u001b[1;32m     42\u001b[0m     all_movies \u001b[38;5;241m=\u001b[39m init_soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshowing-soon\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/bs4/__init__.py:315\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(markup, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m):        \u001b[38;5;66;03m# It's a file-type object.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     markup \u001b[38;5;241m=\u001b[39m markup\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 315\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    316\u001b[0m         (\u001b[38;5;28misinstance\u001b[39m(markup, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m markup)\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(markup, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m markup)\n\u001b[1;32m    318\u001b[0m ):\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m# Issue warnings for a couple beginner problems\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# involving passing non-markup to Beautiful Soup.\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;66;03m# Beautiful Soup will still parse the input as markup,\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;66;03m# since that is sometimes the intended behavior.\u001b[39;00m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_markup_is_url(markup):\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_markup_resembles_filename(markup)                \n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import concurrent.futures\n",
    "\n",
    "def fetch_content(url):\n",
    "    try:\n",
    "        # Add headers to make the request look more like a browser\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        # requests.get是线程安全的\n",
    "        resp = requests.get(url, headers=headers)\n",
    "        resp.raise_for_status()\n",
    "        if resp.text:\n",
    "            print('Read {} from {}'.format(len(resp.text), url))\n",
    "            return resp.text\n",
    "        else:\n",
    "            print('Empty response from {}'.format(url))\n",
    "            return None\n",
    "    except requests.RequestException as e:\n",
    "        print('Error fetching {}: {}'.format(url, e))\n",
    "        return None\n",
    "\n",
    "def crawl_movie(url):\n",
    "    init_page = fetch_content(url)\n",
    "    init_soup = bs4.BeautifulSoup(init_page, 'lxml')\n",
    "\n",
    "    movie_names, urls_to_fetch, movie_dates, pages = [], [], [], []\n",
    "    all_movies = init_soup.find('div', id=\"showing-soon\")\n",
    "    for movie in all_movies.find_all('div', class_='item'):\n",
    "        all_a_tag = movie.find_all('a')\n",
    "        all_li_tag = movie.find_all('li')\n",
    "        # eg:<a href=\"http://example.com/1\">Link 1</a>\n",
    "        movie_name = all_a_tag[1].text\n",
    "        url_to_fetch = all_a_tag[1]['href']\n",
    "        movie_date = all_li_tag[0].text\n",
    "\n",
    "        movie_names.append(movie_name)\n",
    "        urls_to_fetch.append(url_to_fetch)\n",
    "        movie_dates.append(movie_date)\n",
    "\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        pages.extend(executor.map(fetch_content, urls_to_fetch))\n",
    "    \n",
    "    for movie_name, movie_date, page in zip(movie_names, movie_dates, pages):\n",
    "        soup_item = bs4.BeautifulSoup(page, 'lxml')\n",
    "        img_tag = soup_item.find(\"img\")\n",
    "        print('{} {} {}'.format(movie_name, movie_date, img_tag['src']))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://movie.douban.com/cinema/later/beijing/\"\n",
    "    crawl_movie(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download 15 sites in 0.0686110001988709 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnProcess-95:\n",
      "Process SpawnProcess-96:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/concurrent/futures/process.py\", line 251, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'download_one' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/concurrent/futures/process.py\", line 251, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'download_one' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import requests\n",
    "import threading\n",
    "import time\n",
    "\n",
    "def download_one(url):\n",
    "    resp = requests.get(url)\n",
    "    print('Read {} from {}'.format(len(resp.content), url))\n",
    "\n",
    "\n",
    "def download_all(sites):\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        executor.map(download_one, sites)\n",
    "\n",
    "def main():\n",
    "    sites = [\n",
    "        'https://en.wikipedia.org/wiki/Portal:Arts',\n",
    "        'https://en.wikipedia.org/wiki/Portal:History',\n",
    "        'https://en.wikipedia.org/wiki/Portal:Society',\n",
    "        'https://en.wikipedia.org/wiki/Portal:Biography',\n",
    "        'https://en.wikipedia.org/wiki/Portal:Mathematics',\n",
    "        'https://en.wikipedia.org/wiki/Portal:Technology',\n",
    "        'https://en.wikipedia.org/wiki/Portal:Geography',\n",
    "        'https://en.wikipedia.org/wiki/Portal:Science',\n",
    "        'https://en.wikipedia.org/wiki/Computer_science',\n",
    "        'https://en.wikipedia.org/wiki/Python_(programming_language)',\n",
    "        'https://en.wikipedia.org/wiki/Java_(programming_language)',\n",
    "        'https://en.wikipedia.org/wiki/PHP',\n",
    "        'https://en.wikipedia.org/wiki/Node.js',\n",
    "        'https://en.wikipedia.org/wiki/The_C_Programming_Language',\n",
    "        'https://en.wikipedia.org/wiki/Go_(programming_language)'\n",
    "    ]\n",
    "    start_time = time.perf_counter()\n",
    "    download_all(sites)\n",
    "    end_time = time.perf_counter()\n",
    "    print('Download {} sites in {} seconds'.format(len(sites), end_time - start_time))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 99349 from https://movie.douban.com/cinema/later/beijing/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnProcess-3:\n",
      "Process SpawnProcess-1:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/concurrent/futures/process.py\", line 251, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'fetch_content' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/concurrent/futures/process.py\", line 251, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'fetch_content' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnProcess-4:\n",
      "Traceback (most recent call last):\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     52\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://movie.douban.com/cinema/later/beijing/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 53\u001b[0m     \u001b[43mcrawl_movie\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 44\u001b[0m, in \u001b[0;36mcrawl_movie\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     40\u001b[0m     movie_dates\u001b[38;5;241m.\u001b[39mappend(movie_date)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mProcessPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m---> 44\u001b[0m     \u001b[43mpages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfetch_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murls_to_fetch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m movie_name, movie_date, page \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(movie_names, movie_dates, pages):\n\u001b[1;32m     47\u001b[0m     soup_item \u001b[38;5;241m=\u001b[39m bs4\u001b[38;5;241m.\u001b[39mBeautifulSoup(page, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlxml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/concurrent/futures/process.py:642\u001b[0m, in \u001b[0;36m_chain_from_iterable_of_lists\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chain_from_iterable_of_lists\u001b[39m(iterable):\n\u001b[1;32m    637\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;124;03m    Specialized implementation of itertools.chain.from_iterable.\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;124;03m    Each item in *iterable* should be a list.  This function is\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;124;03m    careful not to keep references to yielded objects.\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 642\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43melement\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mwhile\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/concurrent/futures/_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/concurrent/futures/_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/concurrent/futures/_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m: A process in the process pool was terminated abruptly while the future was running or pending."
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import concurrent.futures\n",
    "\n",
    "def fetch_content(url):\n",
    "    try:\n",
    "        # Add headers to make the request look more like a browser\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        # requests.get是线程安全的\n",
    "        resp = requests.get(url, headers=headers)\n",
    "        resp.raise_for_status()\n",
    "        if resp.text:\n",
    "            print('Read {} from {}'.format(len(resp.text), url))\n",
    "            return resp.text\n",
    "        else:\n",
    "            print('Empty response from {}'.format(url))\n",
    "            return None\n",
    "    except requests.RequestException as e:\n",
    "        print('Error fetching {}: {}'.format(url, e))\n",
    "        return None\n",
    "\n",
    "def crawl_movie(url):\n",
    "    init_page = fetch_content(url)\n",
    "    init_soup = bs4.BeautifulSoup(init_page, 'lxml')\n",
    "\n",
    "    movie_names, urls_to_fetch, movie_dates, pages = [], [], [], []\n",
    "    all_movies = init_soup.find('div', id=\"showing-soon\")\n",
    "    for movie in all_movies.find_all('div', class_='item'):\n",
    "        all_a_tag = movie.find_all('a')\n",
    "        all_li_tag = movie.find_all('li')\n",
    "        # eg:<a href=\"http://example.com/1\">Link 1</a>\n",
    "        movie_name = all_a_tag[1].text\n",
    "        url_to_fetch = all_a_tag[1]['href']\n",
    "        movie_date = all_li_tag[0].text\n",
    "\n",
    "        movie_names.append(movie_name)\n",
    "        urls_to_fetch.append(url_to_fetch)\n",
    "        movie_dates.append(movie_date)\n",
    "\n",
    "\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        pages.extend(executor.map(fetch_content, urls_to_fetch))\n",
    "    \n",
    "    for movie_name, movie_date, page in zip(movie_names, movie_dates, pages):\n",
    "        soup_item = bs4.BeautifulSoup(page, 'lxml')\n",
    "        img_tag = soup_item.find(\"img\")\n",
    "        print('{} {} {}'.format(movie_name, movie_date, img_tag['src']))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://movie.douban.com/cinema/later/beijing/\"\n",
    "    crawl_movie(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### futures\n",
    "- executor.submit(func),将返回创建的future实例\n",
    "- done(),非阻塞,立即返回查询结果\n",
    "- add_done_callback(fn),future执行完成,回调对应函数\n",
    "- result(),future执行完成后返回结果\n",
    "- as_completed(fs),针对给定的 future 迭代器 fs，在其完成后，返回完成后的迭代器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error occurred: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Download 15 sites in 0.11836389999371022 seconds\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def download_one(url):\n",
    "    try:\n",
    "        resp = requests.get(url)\n",
    "        print('Read {} from {}'.format(len(resp.content), url))\n",
    "    except requests.RequestException as e:\n",
    "        print('Error fetching {}: {}'.format(url, e))\n",
    "        return None\n",
    "\n",
    "def download_all(sites):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        to_do = []\n",
    "        for site in sites:\n",
    "            future = executor.submit(download_one, site)\n",
    "            to_do.append(future)\n",
    "            \n",
    "        try:\n",
    "            for future in concurrent.futures.as_completed(to_do):\n",
    "                future.result()\n",
    "        except concurrent.futures.TimeoutError as e:\n",
    "            print(f\"Error during parallel processing: {e}\")\n",
    "        except concurrent.futures.TimeoutError:\n",
    "            print('Request timed out')\n",
    "        except concurrent.futures.CancelledError:\n",
    "            print('Request was cancelled')\n",
    "        except requests.RequestException as e:\n",
    "            print(f'Request failed: {e}')\n",
    "        except Exception as e:\n",
    "            print(f'Unexpected error occurred: {e}')\n",
    "def main():\n",
    "    sites = [\n",
    "        'https://en.wikipedia.org/wiki/Portal:Arts',\n",
    "        'https://en.wikipedia.org/wiki/Portal:History',\n",
    "        'https://en.wikipedia.org/wiki/Portal:Society',\n",
    "        'https://en.wikipedia.org/wiki/Portal:Biography',\n",
    "        'https://en.wikipedia.org/wiki/Portal:Mathematics',\n",
    "        'https://en.wikipedia.org/wiki/Portal:Technology',\n",
    "        'https://en.wikipedia.org/wiki/Portal:Geography',\n",
    "        'https://en.wikipedia.org/wiki/Portal:Science',\n",
    "        'https://en.wikipedia.org/wiki/Computer_science',\n",
    "        'https://en.wikipedia.org/wiki/Python_(programming_language)',\n",
    "        'https://en.wikipedia.org/wiki/Java_(programming_language)',\n",
    "        'https://en.wikipedia.org/wiki/PHP',\n",
    "        'https://en.wikipedia.org/wiki/Node.js',\n",
    "        'https://en.wikipedia.org/wiki/The_C_Programming_Language',\n",
    "        'https://en.wikipedia.org/wiki/Go_(programming_language)'\n",
    "    ]\n",
    "    start_time = time.perf_counter()\n",
    "    download_all(sites)\n",
    "    end_time = time.perf_counter()\n",
    "    print('Download {} sites in {} seconds'.format(len(sites), end_time - start_time))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
